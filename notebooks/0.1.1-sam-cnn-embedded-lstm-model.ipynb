{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 22:24:49.966826: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-22 22:24:49.993294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742682290.021639   20446 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742682290.027963   20446 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742682290.044280   20446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742682290.044302   20446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742682290.044305   20446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742682290.044306   20446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-22 22:24:50.049384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from dvclive import Live\n",
    "from dvclive.keras import DVCLiveCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get root directory\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent  # Assumes notebook is in a subdirectory\n",
    "ORI_DATA_PATH = os.path.join(ROOT_DIR, 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac02e3b4862e4733bc922e72ae52f9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Video\n",
    "\n",
    "ds = load_dataset(\"videofolder\", data_dir=ORI_DATA_PATH).cast_column(\"video\", Video(decode=False))\n",
    "ds_train_devtest = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "ds_devtest = ds_train_devtest['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': ds_train_devtest['train'],\n",
    "    'valid': ds_devtest['train'],\n",
    "    'test': ds_devtest['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'apa kabar', 1: 'ayo jalan-jalan', 2: 'jaga kesehatan', 3: 'kamu mau kemana', 4: 'kamu tinggal dimana', 5: 'mau pesan apa', 6: 'nama kamu siapa', 7: 'salam kenal', 8: 'sama-sama', 9: 'sampai jumpa lagi', 10: 'saya minta maaf', 11: 'sekarang jam berapa', 12: 'selamat malam', 13: 'selamat pagi', 14: 'selamat siang', 15: 'terima kasih'}\n"
     ]
    }
   ],
   "source": [
    "label_feature = ds['train'].features['label']\n",
    "label_names = label_feature.names\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = os.path.join(ROOT_DIR, 'data', 'interim', 'trim_padded_ds_random')\n",
    "MAX_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "# import decord\n",
    "# from decord import VideoReader, cpu\n",
    "# decord.bridge.set_bridge('tensorflow')\n",
    "\n",
    "def trim_pad(example, max_seq_length):\n",
    "    # vr = VideoReader(example[\"video\"][\"path\"], ctx=cpu(0))\n",
    "    frames = tf.io.read_file(example[\"video\"][\"path\"])\n",
    "    frames = tfio.experimental.ffmpeg.decode_video(frames)\n",
    "    frames = frames[:max_seq_length]\n",
    "    video_length = frames.shape[0]\n",
    "\n",
    "    # Trim or pad frames to MAX_SEQ_LENGTH\n",
    "    if video_length == max_seq_length:\n",
    "        # create mask\n",
    "        mask = tf.ones(max_seq_length, dtype=tf.bool)\n",
    "    else:\n",
    "        # create mask and pad if too short\n",
    "        mask = tf.zeros(max_seq_length, dtype=tf.bool)\n",
    "        mask = tf.tensor_scatter_nd_update(\n",
    "            mask,\n",
    "            tf.reshape(tf.range(video_length), [-1, 1]),\n",
    "            tf.ones(video_length, dtype=tf.bool)\n",
    "        )\n",
    "        # Pad with zeros\n",
    "        padding = tf.zeros((max_seq_length - video_length, *frames.shape[1:]), dtype=frames.dtype)\n",
    "        frames = tf.concat([frames, padding], axis=0)\n",
    "\n",
    "    return {\n",
    "        \"frames\": frames,\n",
    "        \"mask\": mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea14104afc994a469b46f06d41e47062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 21:31:29.016738: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/ffmpeg_ops.py:43: UserWarning: could not load libtensorflow_io_ffmpeg_4.2.so: unable to open file: libtensorflow_io_ffmpeg_4.2.so, from paths: ['/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_ffmpeg_4.2.so']\n",
      "caused by: ['/lib/x86_64-linux-gnu/libgobject-2.0.so.0: undefined symbol: ffi_type_uint32, version LIBFFI_BASE_7.0']\n",
      "  warnings.warn(f\"could not load {library}: {e}\")\n",
      "/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/ffmpeg_ops.py:43: UserWarning: could not load libtensorflow_io_ffmpeg_3.4.so: unable to open file: libtensorflow_io_ffmpeg_3.4.so, from paths: ['/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_ffmpeg_3.4.so']\n",
      "caused by: ['libavformat.so.57: cannot open shared object file: No such file or directory']\n",
      "  warnings.warn(f\"could not load {library}: {e}\")\n",
      "/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/ffmpeg_ops.py:43: UserWarning: could not load libtensorflow_io_ffmpeg_2.8.so: unable to open file: libtensorflow_io_ffmpeg_2.8.so, from paths: ['/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_ffmpeg_2.8.so']\n",
      "caused by: ['libavcodec-ffmpeg.so.56: cannot open shared object file: No such file or directory']\n",
      "  warnings.warn(f\"could not load {library}: {e}\")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "('could not find ffmpeg after search through ', ['libtensorflow_io_ffmpeg_4.2.so', 'libtensorflow_io_ffmpeg_3.4.so', 'libtensorflow_io_ffmpeg_2.8.so'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m ds\u001b[38;5;241m.\u001b[39msave_to_disk(PROCESSED_DATA_PATH)\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/dataset_dict.py:941\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    939\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 941\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    962\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3070\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3071\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3072\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3073\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3074\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3075\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3076\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3492\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3491\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3492\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3493\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3494\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3466\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3389\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3389\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtrim_pad\u001b[0;34m(example, max_seq_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrim_pad\u001b[39m(example, max_seq_length):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# vr = VideoReader(example[\"video\"][\"path\"], ctx=cpu(0))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     frames \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_file(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mtfio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     frames \u001b[38;5;241m=\u001b[39m frames[:max_seq_length]\n\u001b[1;32m     12\u001b[0m     video_length \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/experimental/ffmpeg_ops.py:28\u001b[0m, in \u001b[0;36mdecode_video\u001b[0;34m(content, index, name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_video\u001b[39m(content, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode video stream from a video file.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m      value: A `uint8` Tensor.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_io\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         ffmpeg_ops,\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ffmpeg_ops\u001b[38;5;241m.\u001b[39mio_ffmpeg_decode_video(content, index, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/ffmpeg_ops.py:48\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not find ffmpeg after search through \u001b[39m\u001b[38;5;124m\"\u001b[39m, p)\n\u001b[0;32m---> 48\u001b[0m _ffmpeg_ops, _decode_ops \u001b[38;5;241m=\u001b[39m \u001b[43m_load_libraries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtensorflow_io_ffmpeg_4.2.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtensorflow_io_ffmpeg_3.4.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtensorflow_io_ffmpeg_2.8.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m io_ffmpeg_readable_init \u001b[38;5;241m=\u001b[39m _ffmpeg_ops\u001b[38;5;241m.\u001b[39mio_ffmpeg_readable_init\n\u001b[1;32m     57\u001b[0m io_ffmpeg_readable_spec \u001b[38;5;241m=\u001b[39m _ffmpeg_ops\u001b[38;5;241m.\u001b[39mio_ffmpeg_readable_spec\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow_io/python/ops/ffmpeg_ops.py:45\u001b[0m, in \u001b[0;36m_load_libraries\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     43\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibrary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not find ffmpeg after search through \u001b[39m\u001b[38;5;124m\"\u001b[39m, p)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ('could not find ffmpeg after search through ', ['libtensorflow_io_ffmpeg_4.2.so', 'libtensorflow_io_ffmpeg_3.4.so', 'libtensorflow_io_ffmpeg_2.8.so'])"
     ]
    }
   ],
   "source": [
    "ds = ds.map(\n",
    "    trim_pad,\n",
    "    MAX_SEQ_LEN,\n",
    "    batched=False,\n",
    "    remove_columns=[\"video\"]\n",
    ")\n",
    "\n",
    "ds.save_to_disk(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model (Preprocessing, feature extraction, sequence recognition, and classification included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim and extend\n",
    "import tensorflow as tf\n",
    "\n",
    "class VideoTrimmerExtender(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_seq_length, **kwargs):\n",
    "        super(VideoTrimmerExtender, self).__init__(**kwargs)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, frames):\n",
    "        video_shape = tf.shape(frames)\n",
    "\n",
    "        if len(frames.shape) == 5:  # (batch, frames, height, width, channels)\n",
    "            batch_size = video_shape[0]\n",
    "            video_length = video_shape[1]\n",
    "            height = video_shape[2]\n",
    "            width = video_shape[3]\n",
    "            channels = video_shape[4]\n",
    "\n",
    "            # if video is longer\n",
    "            if video_length >= self.max_seq_length:\n",
    "                frames = frames[:, :self.max_seq_length]\n",
    "                mask = tf.ones([batch_size, self.max_seq_length], dtype=tf.bool)\n",
    "\n",
    "            else:\n",
    "                mask = tf.zeros([batch_size, self.max_seq_length], dtype=tf.bool)\n",
    "\n",
    "                # Create indices for the valid frames using meshgrid\n",
    "                batch_indices = tf.tile(\n",
    "                    tf.expand_dims(tf.range(batch_size), 1),\n",
    "                    [1, video_length]\n",
    "                )\n",
    "                time_indices = tf.tile(\n",
    "                    tf.expand_dims(tf.range(video_length), 0),\n",
    "                    [batch_size, 1]\n",
    "                )\n",
    "\n",
    "                # Stack indices to create coordinate pairs\n",
    "                indices = tf.stack([\n",
    "                    tf.reshape(batch_indices, [-1]),\n",
    "                    tf.reshape(time_indices, [-1])\n",
    "                ], axis=1)\n",
    "\n",
    "                # Update mask\n",
    "                mask = tf.tensor_scatter_nd_update(\n",
    "                    mask,\n",
    "                    indices,\n",
    "                    tf.ones(batch_size * video_length, dtype=tf.bool)\n",
    "                )\n",
    "\n",
    "                # Create padding with all 5 dimensions\n",
    "                padding_shape = [\n",
    "                    batch_size,                   # batch\n",
    "                    self.max_seq_length - video_length,  # time\n",
    "                    height,                       # height\n",
    "                    width,                        # width\n",
    "                    channels                      # channels\n",
    "                ]\n",
    "                padding = tf.zeros(padding_shape, dtype=frames.dtype)\n",
    "                # Concatenate along time dimension (axis=1), NOT batch dimension\n",
    "                frames = tf.concat([frames, padding], axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Expected input shape with 5 dimensions (batch, frames, height, width, channels)\")\n",
    "\n",
    "        return frames, mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Return shape for frames and mask\n",
    "        frames_shape = (None,) + input_shape[1:]\n",
    "        mask_shape = (self.max_seq_length,)\n",
    "        return [frames_shape, mask_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center cropping\n",
    "\n",
    "class CenterSquareCrop(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenterSquareCrop, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) != 4:  # (frames, height, width, channels)\n",
    "            raise ValueError(\"Expected input shape with 4 dimensions (frames, height, width, channels)\")\n",
    "\n",
    "        # Get the dynamic shape of the input image\n",
    "        shape = tf.shape(inputs)\n",
    "        height = shape[1]\n",
    "        width = shape[2]\n",
    "        # Determine the side length of the largest possible central square\n",
    "        crop_size = tf.minimum(height, width)\n",
    "        # Compute offsets for centering the crop\n",
    "        offset_height = (height - crop_size) // 2\n",
    "        offset_width = (width - crop_size) // 2\n",
    "        # Crop the central square from each image in the batch\n",
    "        return tf.image.crop_to_bounding_box(inputs, offset_height, offset_width, crop_size, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self, image_size, **kwargs):\n",
    "        super(CustomPreprocessing, self).__init__(**kwargs)\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # create all layers during initialization\n",
    "        self.random_brightness = tf.keras.layers.RandomBrightness(0.2)\n",
    "        self.random_contrast = tf.keras.layers.RandomContrast(0.2)\n",
    "        self.center_crop = CenterSquareCrop()\n",
    "        self.resize = tf.keras.layers.Resizing(image_size, image_size)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 1. Transpose inputs to (batch, height, width, channels * time)\n",
    "        shape = tf.shape(inputs)\n",
    "        batch = shape[0]\n",
    "        time = shape[1]\n",
    "        width = shape[2]\n",
    "        height = shape[3]\n",
    "        channels = shape[4]\n",
    "        x = tf.transpose(inputs, [0, 2, 3, 4, 1])\n",
    "        # combine channels and time\n",
    "        x = tf.reshape(\n",
    "            x,\n",
    "            (batch, width, height, time * channels)\n",
    "        )\n",
    "\n",
    "        # 2. Apply Preprocessing\n",
    "        if training:\n",
    "            x = self.random_brightness(x)\n",
    "            x = self.random_contrast(x)\n",
    "        x = self.center_crop(x)\n",
    "        x = self.resize(x)\n",
    "\n",
    "        # 3. Transpose back to (batch, time, height, width, channels)\n",
    "        new_shape = tf.shape(x)\n",
    "        new_batch = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        new_height = new_shape[2]\n",
    "        new_channels = new_shape[3]\n",
    "        # Calculate time dimension for reshape\n",
    "        time_dim = new_channels // channels\n",
    "\n",
    "        x = tf.reshape(\n",
    "            x,\n",
    "            (new_batch, new_width, new_height, channels, time_dim)\n",
    "        )\n",
    "        x = tf.transpose(x, [0, 4, 1, 2, 3])\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch, time, _, _, _ = input_shape\n",
    "        return (batch, time, self.image_size, self.image_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFeatureExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VideoFeatureExtractor, self).__init__(**kwargs)\n",
    "        self.feature_extractor = tf.keras.applications.InceptionV3(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling='avg',\n",
    "            input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "        )\n",
    "\n",
    "    def call(self, frames):\n",
    "        # Preprocess frames\n",
    "        frames = tf.keras.applications.inception_v3.preprocess_input(frames)\n",
    "        # Extract features from frames\n",
    "        features = self.feature_extractor(frames)\n",
    "        return features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'preprocessing', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "frames_inputs = tf.keras.layers.Input(shape=(MAX_SEQ_LEN, None, None, 3), name='frames')\n",
    "mask_inputs = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), name='mask')\n",
    "\n",
    "\n",
    "# preprocessing process =====\n",
    "# frames, mask = VideoTrimmerExtender(max_seq_length=MAX_SEQ_LEN)(inputs)\n",
    "\n",
    "x = CustomPreprocessing(image_size=IMAGE_SIZE, name=\"preprocessing\")(frames_inputs)\n",
    "# ============================\n",
    "\n",
    "# feature extraction process =====\n",
    "# features output shape is (time, features)\n",
    "feature_extractor = VideoFeatureExtractor()\n",
    "features = tf.keras.layers.TimeDistributed(feature_extractor,\n",
    "                                           name=\"cnn_feature_extractor\")(x)\n",
    "# ================================\n",
    "\n",
    "# lstm process\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(features, mask=mask_inputs)\n",
    "\n",
    "# classification process\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.LSTM(128)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation=\"elu\")(x)\n",
    "x = tf.keras.layers.Dense(512, activation='elu')(x)\n",
    "output = tf.keras.layers.Dense(len(label_names), activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[frames_inputs, mask_inputs], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ frames (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ preprocessing       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ frames[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomPreprocessi…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_feature_extrac… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │ preprocessing[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,720,640</span> │ cnn_feature_extr… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">328,192</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,096</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,208</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ frames (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;45mNone\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ preprocessing       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m299\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ frames[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mCustomPreprocessi…\u001b[0m │ \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_feature_extrac… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m2048\u001b[0m) │ \u001b[38;5;34m21,802,784\u001b[0m │ preprocessing[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mask (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m4,720,640\u001b[0m │ cnn_feature_extr… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m328,192\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m132,096\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m524,800\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m8,208\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,516,720</span> (104.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,516,720\u001b[0m (104.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,482,288</span> (104.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,482,288\u001b[0m (104.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,432</span> (134.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,432\u001b[0m (134.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 500\n",
    "MODEL_SAVE_PATH = os.path.join(ROOT_DIR, 'models', 'inceptionv3-lstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds_train_devtest = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "# ds_devtest = ds_train_devtest['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# ds = DatasetDict({\n",
    "#     'train': ds_train_devtest['train'],\n",
    "#     'valid': ds_devtest['train'],\n",
    "#     'test': ds_devtest['test']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1080, 1620, 3), dtype=uint8, numpy=\n",
       "array([[[[234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         ...,\n",
       "         [137, 135, 136],\n",
       "         [138, 136, 137],\n",
       "         [139, 137, 138]],\n",
       "\n",
       "        [[234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         ...,\n",
       "         [137, 135, 136],\n",
       "         [138, 136, 137],\n",
       "         [139, 137, 138]],\n",
       "\n",
       "        [[234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         [234, 227, 222],\n",
       "         ...,\n",
       "         [137, 135, 136],\n",
       "         [138, 136, 137],\n",
       "         [139, 137, 138]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[207, 200, 193],\n",
       "         [207, 200, 193],\n",
       "         [207, 200, 193],\n",
       "         ...,\n",
       "         [ 81,  43,  23],\n",
       "         [ 80,  42,  20],\n",
       "         [ 80,  42,  20]],\n",
       "\n",
       "        [[208, 201, 194],\n",
       "         [208, 201, 194],\n",
       "         [208, 201, 194],\n",
       "         ...,\n",
       "         [ 78,  40,  20],\n",
       "         [ 77,  39,  17],\n",
       "         [ 77,  39,  17]],\n",
       "\n",
       "        [[163, 156, 149],\n",
       "         [163, 156, 149],\n",
       "         [163, 156, 149],\n",
       "         ...,\n",
       "         [ 71,  33,  13],\n",
       "         [ 73,  35,  13],\n",
       "         [ 73,  35,  13]]],\n",
       "\n",
       "\n",
       "       [[[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [128, 133, 129],\n",
       "         [133, 135, 132],\n",
       "         [134, 136, 133]],\n",
       "\n",
       "        [[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [131, 136, 132],\n",
       "         [135, 137, 134],\n",
       "         [136, 138, 135]],\n",
       "\n",
       "        [[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [132, 137, 133],\n",
       "         [135, 137, 134],\n",
       "         [136, 138, 135]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[206, 199, 194],\n",
       "         [206, 199, 194],\n",
       "         [206, 199, 194],\n",
       "         ...,\n",
       "         [ 82,  39,  20],\n",
       "         [ 82,  39,  20],\n",
       "         [ 84,  41,  22]],\n",
       "\n",
       "        [[207, 200, 195],\n",
       "         [207, 200, 195],\n",
       "         [207, 200, 195],\n",
       "         ...,\n",
       "         [ 80,  40,  20],\n",
       "         [ 80,  40,  20],\n",
       "         [ 82,  42,  22]],\n",
       "\n",
       "        [[162, 155, 150],\n",
       "         [162, 155, 150],\n",
       "         [162, 155, 150],\n",
       "         ...,\n",
       "         [ 73,  33,  13],\n",
       "         [ 73,  33,  13],\n",
       "         [ 73,  33,  13]]],\n",
       "\n",
       "\n",
       "       [[[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [130, 128, 129],\n",
       "         [134, 129, 128],\n",
       "         [135, 130, 129]],\n",
       "\n",
       "        [[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [134, 132, 133],\n",
       "         [140, 135, 134],\n",
       "         [140, 135, 134]],\n",
       "\n",
       "        [[236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [136, 134, 135],\n",
       "         [140, 135, 134],\n",
       "         [140, 135, 134]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[206, 199, 194],\n",
       "         [206, 199, 194],\n",
       "         [207, 200, 195],\n",
       "         ...,\n",
       "         [ 82,  42,  20],\n",
       "         [ 82,  42,  20],\n",
       "         [ 82,  42,  20]],\n",
       "\n",
       "        [[207, 200, 195],\n",
       "         [207, 200, 195],\n",
       "         [207, 200, 195],\n",
       "         ...,\n",
       "         [ 82,  42,  22],\n",
       "         [ 83,  43,  21],\n",
       "         [ 84,  44,  22]],\n",
       "\n",
       "        [[162, 155, 150],\n",
       "         [162, 155, 150],\n",
       "         [162, 155, 150],\n",
       "         ...,\n",
       "         [ 75,  35,  15],\n",
       "         [ 76,  36,  14],\n",
       "         [ 77,  37,  15]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[235, 228, 221],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [136, 128, 133],\n",
       "         [138, 131, 133],\n",
       "         [140, 133, 135]],\n",
       "\n",
       "        [[235, 228, 221],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [137, 129, 134],\n",
       "         [141, 134, 136],\n",
       "         [141, 134, 136]],\n",
       "\n",
       "        [[235, 228, 221],\n",
       "         [236, 229, 222],\n",
       "         [236, 229, 222],\n",
       "         ...,\n",
       "         [138, 130, 135],\n",
       "         [142, 135, 137],\n",
       "         [143, 136, 138]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[205, 198, 193],\n",
       "         [205, 198, 193],\n",
       "         [208, 198, 194],\n",
       "         ...,\n",
       "         [ 82,  45,  16],\n",
       "         [ 82,  45,  16],\n",
       "         [ 82,  45,  16]],\n",
       "\n",
       "        [[204, 197, 192],\n",
       "         [204, 197, 192],\n",
       "         [208, 198, 194],\n",
       "         ...,\n",
       "         [ 81,  44,  19],\n",
       "         [ 82,  45,  20],\n",
       "         [ 82,  45,  20]],\n",
       "\n",
       "        [[161, 154, 149],\n",
       "         [161, 154, 149],\n",
       "         [164, 154, 150],\n",
       "         ...,\n",
       "         [ 71,  34,   9],\n",
       "         [ 73,  36,  11],\n",
       "         [ 74,  37,  12]]],\n",
       "\n",
       "\n",
       "       [[[236, 226, 220],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [131, 124, 126],\n",
       "         [134, 127, 126],\n",
       "         [137, 130, 129]],\n",
       "\n",
       "        [[236, 226, 220],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [133, 126, 128],\n",
       "         [136, 129, 128],\n",
       "         [138, 131, 130]],\n",
       "\n",
       "        [[236, 226, 220],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [134, 127, 129],\n",
       "         [137, 130, 129],\n",
       "         [140, 133, 132]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[205, 198, 191],\n",
       "         [205, 198, 191],\n",
       "         [206, 199, 194],\n",
       "         ...,\n",
       "         [ 80,  43,  14],\n",
       "         [ 80,  43,  14],\n",
       "         [ 80,  43,  14]],\n",
       "\n",
       "        [[204, 197, 190],\n",
       "         [204, 197, 190],\n",
       "         [206, 199, 194],\n",
       "         ...,\n",
       "         [ 80,  43,  18],\n",
       "         [ 80,  43,  18],\n",
       "         [ 80,  43,  18]],\n",
       "\n",
       "        [[161, 154, 147],\n",
       "         [161, 154, 147],\n",
       "         [162, 155, 150],\n",
       "         ...,\n",
       "         [ 69,  32,   7],\n",
       "         [ 69,  32,   7],\n",
       "         [ 69,  32,   7]]],\n",
       "\n",
       "\n",
       "       [[[237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [133, 126, 128],\n",
       "         [137, 130, 129],\n",
       "         [140, 133, 132]],\n",
       "\n",
       "        [[237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [134, 127, 129],\n",
       "         [137, 130, 129],\n",
       "         [140, 133, 132]],\n",
       "\n",
       "        [[237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         [237, 227, 221],\n",
       "         ...,\n",
       "         [134, 127, 129],\n",
       "         [136, 129, 128],\n",
       "         [137, 130, 129]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[206, 199, 194],\n",
       "         [206, 199, 194],\n",
       "         [207, 200, 195],\n",
       "         ...,\n",
       "         [ 85,  43,  17],\n",
       "         [ 85,  43,  17],\n",
       "         [ 85,  43,  17]],\n",
       "\n",
       "        [[205, 198, 193],\n",
       "         [205, 198, 193],\n",
       "         [207, 200, 195],\n",
       "         ...,\n",
       "         [ 86,  44,  18],\n",
       "         [ 86,  44,  18],\n",
       "         [ 86,  44,  18]],\n",
       "\n",
       "        [[162, 155, 150],\n",
       "         [161, 154, 149],\n",
       "         [163, 156, 151],\n",
       "         ...,\n",
       "         [ 75,  33,   7],\n",
       "         [ 75,  33,   7],\n",
       "         [ 75,  33,   7]]]], dtype=uint8)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr = VideoReader(ds[\"train\"][0][\"video\"][\"path\"])\n",
    "vr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "from decord import VideoReader\n",
    "\n",
    "def video_generator(dataset, max_seq_length):\n",
    "    \"\"\"A generator that yields video frames and labels.\"\"\"\n",
    "    for example in dataset:\n",
    "        vr = VideoReader(example[\"video\"][\"path\"])\n",
    "        frames = vr[:max_seq_length]\n",
    "        frames = tf.convert_to_tensor(frames.asnumpy())\n",
    "\n",
    "        video_length = frames.shape[0]\n",
    "\n",
    "        # Trim or pad frames to MAX_SEQ_LENGTH\n",
    "        if video_length == max_seq_length:\n",
    "            # create mask\n",
    "            mask = tf.ones(max_seq_length, dtype=tf.bool)\n",
    "        else:\n",
    "            # create mask and pad if too short\n",
    "            mask = tf.zeros(max_seq_length, dtype=tf.bool)\n",
    "            mask = tf.tensor_scatter_nd_update(\n",
    "                mask,\n",
    "                tf.reshape(tf.range(video_length), [-1, 1]),\n",
    "                tf.ones(video_length, dtype=tf.bool)\n",
    "            )\n",
    "            # Pad with zeros\n",
    "            padding = tf.zeros((max_seq_length - video_length, *frames.shape[1:]), dtype=frames.dtype)\n",
    "            frames = tf.concat([frames, padding], axis=0)\n",
    "\n",
    "        yield (frames, mask), example[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: video_generator(ds['train'], MAX_SEQ_LEN),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.uint8), tf.TensorSpec(shape=(None,), dtype=tf.bool)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: video_generator(ds['valid'], MAX_SEQ_LEN),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.uint8), tf.TensorSpec(shape=(None,), dtype=tf.bool)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: video_generator(ds['test'], MAX_SEQ_LEN),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.uint8), tf.TensorSpec(shape=(None,), dtype=tf.bool)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 22:32:12.647299: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14117750193991197659\n",
      "2025-03-22 22:32:12.647326: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14516317140644479754\n",
      "2025-03-22 22:32:12.647333: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 11309262624434904116\n",
      "2025-03-22 22:32:12.647805: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5245893010251101508\n",
      "WARNING: The following untracked files were present in the workspace before saving but will not be included in the experiment commit:\n",
      "\t.python-version, notebooks/0.1.1-sam-cnn-embedded-lstm-model.ipynb\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_20446/3578193721.py\", line 8, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_20446/3578193721.py\", line 8, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Cannot batch tensors with different shapes in component 0. First element had shape [100,1080,1620,3] and element 6 had shape [100,720,1280,3].\n\t [[{{node IteratorGetNext}}]]\n\t [[StatefulPartitionedCall/Shape/_8]]\n  (1) INVALID_ARGUMENT:  Cannot batch tensors with different shapes in component 0. First element had shape [100,1080,1620,3] and element 6 had shape [100,720,1280,3].\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_68763]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                                     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                                                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Live(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdvclive\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m live:\n\u001b[0;32m----> 8\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDVCLiveCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(MODEL_SAVE_PATH)\n\u001b[1;32m     21\u001b[0m     live\u001b[38;5;241m.\u001b[39mlog_artifact(\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mstr\u001b[39m(MODEL_SAVE_PATH),\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minceptionv3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1e-4\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     27\u001b[0m     )\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_20446/3578193721.py\", line 8, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_20446/3578193721.py\", line 8, in <module>\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/teamspace/studios/this_studio/bisindo-video-recognition/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Cannot batch tensors with different shapes in component 0. First element had shape [100,1080,1620,3] and element 6 had shape [100,720,1280,3].\n\t [[{{node IteratorGetNext}}]]\n\t [[StatefulPartitionedCall/Shape/_8]]\n  (1) INVALID_ARGUMENT:  Cannot batch tensors with different shapes in component 0. First element had shape [100,1080,1620,3] and element 6 had shape [100,720,1280,3].\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_68763]"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                    patience=50,\n",
    "                                                    restore_best_weights=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "\n",
    "with Live(dir=os.path.join(ROOT_DIR, 'dvclive')) as live:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            early_stopping,\n",
    "            DVCLiveCallback(live=live)\n",
    "        ],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "    live.log_artifact(\n",
    "        str(MODEL_SAVE_PATH),\n",
    "        type=\"model\",\n",
    "        name=\"inceptionv3-lstm\",\n",
    "        desc=\"InceptionV3 + LSTM model\",\n",
    "        labels=[\"inceptionv3\", \"lstm\", \"adam\", \"sparse_categorical_crossentropy\", \"1e-4\"],\n",
    "    )\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "    live.log_metric(\"test_loss\", loss)\n",
    "    live.log_metric(\"test_accuracy\", accuracy)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
